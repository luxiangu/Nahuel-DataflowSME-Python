# Tutorial for Cloud Dataflow

This is a collection of tutorial-style Dataflow exercises based on the [Dataflow
gaming
example](https://github.com/GoogleCloudPlatform/DataflowJavaSDK-examples/blob/master/src/main/java8/com/google/cloud/dataflow/examples/complete/game/README.md)
and inspired by the [Beam tutorial](https://github.com/eljefe6a/beamexample).

In the gaming scenario, many users play a fictional game, as members of
different teams, over the course of a day, and their events are logged for
processing.

The exercises either read the batch data from CSV files on GCS or the streaming
data from a [PubSub](https://cloud.google.com/pubsub/) topic (generated by the
included `Injector` program). All exercises write their output to BigQuery.

## Set up your environment

### Tools

1.  [Download and install Git](https://git-scm.com/downloads).
1.  Install Python 2.7. If you use Linux, install it through apt or yum. Otherwise [download it from the website](https://www.python.org/downloads/release/python-2715/).
1.  Install [virtualenv](https://virtualenv.pypa.io/en/stable/).
1.  Install Google [Cloud SDK](https://cloud.google.com/sdk/).
1.  Install an IDE that supports Python (optional).

### Set up a project

1.  Go to [https://cloud.google.com/console](https://cloud.google.com/console).
1.  Enable billing and create a project. For this project:
    1.  Enable Google Dataflow API and BigQuery API.
    1.  Create a GCS bucket and two folders inside the bucket:
        1. A staging folder.
        1. A temp folder.
    1.  Create a BigQuery dataset to store the results.

#### Prepare your environment

1. Create your SME environment. Open a shell (terminal or console) and cd into your home folder. 
There, run:

    ```shell
    $ virtualenv dataflowsme
    $ source dataflowsme/bin/activate
    (dataflowsme) $
    ```

1. Authenticate to Google Cloud using the gcloud command and set the default credentials and 
default project. You will need to replace YOUR-PROJECT-ID with the id of the project 
you created before:

    ```shell
    $ gcloud auth login
    $ gcloud auth application-default login
    $ gcloud config set project YOUR-PROJECT-ID
    ```
    
1. Get the project name with Gcloud and set it as an env variable:
    ```shell
    $ export PROJECT=`gcloud config get-value project`
    ```
    
1. Set other environment variables
    ```shell
    $ export STAGING_FOLDER=gs://<path of the bucket and stagins folder that you created before>
    $ export TEMP_FOLDER=gs://<path of the bucket and temp folder that you created before>
    $ export BIGQUERY_DATASET=<name of the dataset that you created before>
    $ export USER=`whoami`
    ```

### Download the code

Clone the github repository

    ```shell
    $ git clone https://github.com/nahuellofeudo/DataflowSME-Python.git
    $ cd DataflowSME
    ```

## Exercise 0 (prework)

**Goal**: Use the provided Dataflow pipeline to import the input events from a file in GCS to
BigQuery and run simple queries on the result.

**Procedure**:

1.  Compile and run the pipeline:

    ```shell
    $ python2.7 exercise0.py \
        --project=$PROJECT \
        --setup_file=./setup.py \
        --input=gs://dataflow-samples/game/gaming_data1.csv \
        --output_dataset=$BIGQUERY_DATASET \
        --output_table_name=events \
        --runner=DataflowRunner \
        --temp_location=$TEMP_FOLDER \
        --staging_location=$STAGING_FOLDER 
    ```

1.  Open [https://console.cloud.google.com](https://console.cloud.google.com) and navigate to the Dataflow UI.

1.  Once the pipeline finishes (should take about 15-20 minutes), the Job Status
    on the UI changes to Succeeded.

1.  After the pipeline finishes, check the value of `ParseGameEvent/ParseErrors`
    aggregator on the UI. Scroll down in the Summary tab to find it. 

1.  Check the number of distinct teams in the created BigQuery table.

    ```shell
    $ bq query --project_id=$PROJECT \
        'select count(distinct team) from $BIGQUERY_DATASET.events;'
    ```
